{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ixVcgFFjfQP",
   "metadata": {
    "id": "8ixVcgFFjfQP",
    "include-cell-in-app": true
   },
   "source": [
    "# **Steel Defect Detection**\n",
    "In this project, I trained deep learning models for detecting steel surface defects using **VGG-like Model** and **Lightweight EfficientNet U-Net** models.\n",
    "\n",
    "## Summary of Results\n",
    "Both the VGG-like model and the EfficientNet U-Net achieve similar Dice scores (0.90), indicating strong segmentation quality. However, the VGG-like model delivers slightly better IoU (0.62) and recall (0.73), and trains much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3O8rUlAMf56Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1556,
     "status": "ok",
     "timestamp": 1758657946463,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "3O8rUlAMf56Y",
    "outputId": "f8d44403-61a8-4590-ea4a-476428fb29c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f3a55c5-9090-427f-a551-03661546e788",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1758657946473,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "3f3a55c5-9090-427f-a551-03661546e788",
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import datetime\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1sJiMFtVmyZS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1758657946481,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "1sJiMFtVmyZS",
    "outputId": "0d8a0ff5-cfeb-4435-aa30-8c0af57e25cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"TF GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6062b-8a40-4c19-ad82-747c0b0b2ccc",
   "metadata": {
    "id": "0ef6062b-8a40-4c19-ad82-747c0b0b2ccc",
    "include-cell-in-app": true
   },
   "source": [
    "## Read and Explore Data\n",
    "The data used for this project was downloaded from https://www.kaggle.com/competitions/severstal-steel-defect-detection/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01a9802b-ffd1-41b5-b4b9-bfe514c333e1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 374,
     "status": "ok",
     "timestamp": 1758657946857,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "01a9802b-ffd1-41b5-b4b9-bfe514c333e1",
    "include-cell-in-app": true,
    "outputId": "bd8cb98b-4fef-4d85-af68-d81931655e16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7095, 3)\n",
      "Index(['ImageId', 'ClassId', 'EncodedPixels'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv('/content/drive/MyDrive/severstal-steel-defect-detection/train.csv')\n",
    "print (df.shape)\n",
    "print (df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "913c5b4f-b6a3-4ba2-b42d-2f3dbcd621c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1758657947076,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "913c5b4f-b6a3-4ba2-b42d-2f3dbcd621c1",
    "include-cell-in-app": true,
    "outputId": "e369d6e5-35d2-4b95-e8c3-43cf8273b628"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 7095,\n  \"fields\": [\n    {\n      \"column\": \"ImageId\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6666,\n        \"samples\": [\n          \"0934b8bff.jpg\",\n          \"5994c3b58.jpg\",\n          \"ec52fac2d.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ClassId\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3,\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"EncodedPixels\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7095,\n        \"samples\": [\n          \"171129 87 171305 215 171521 255 171777 255 172034 254 172290 253 172546 253 172802 253 173059 252 173315 252 173571 252 173828 251 174084 250 174340 250 174596 250 174853 249 175109 124 309507 14 309709 50 309763 41 309867 148 310018 253 310274 253 310530 253 310786 253 311042 254 311298 254 311596 212 311937 127 312277 43 312433 112 312578 223 312834 223 313090 222 313345 223 313601 167 313857 56 352260 63 352516 188 352772 251 353028 251 353284 251 353541 250 353797 249 354053 249 354309 249 354565 249 354821 249 355077 249 355333 248 355589 248 355845 248 356102 247 356358 247 356614 247 356870 246 357126 246 357444 184 357822 62\",\n          \"313510 2 313561 1 313762 5 313816 3 313993 2 314014 7 314049 4 314071 5 314248 5 314266 9 314302 7 314325 9 314503 6 314518 11 314557 8 314580 10 314759 6 314772 11 314811 9 314835 12 315014 7 315028 10 315066 10 315090 13 315111 15 315269 8 315283 9 315320 12 315345 14 315364 20 315524 9 315539 7 315574 14 315599 17 315617 26 315761 2 315781 7 315794 6 315827 16 315853 48 316015 3 316038 6 316050 7 316081 18 316106 53 316269 5 316295 5 316305 9 316336 19 316360 55 316523 6 316553 3 316561 9 316590 21 316613 58 316777 8 316800 1 316810 2 316816 11 316844 22 316867 60 317031 9 317054 2 317067 1 317072 12 317099 84 317285 10 317308 4 317328 13 317353 86 317540 11 317563 4 317585 13 317608 74 317684 11 317796 10 317817 6 317841 13 317861 75 317942 10 318053 9 318071 7 318098 13 318116 74 318199 9 318309 8 318326 7 318354 14 318372 72 318457 7 318566 7 318582 7 318611 12 318627 73 318714 6 318822 6 318839 5 318867 12 318883 73 318971 5 319078 5 319095 4 319123 11 319139 61 319207 4 319229 3 319335 4 319351 4 319379 10 319394 58 319486 2 319591 3 319607 3 319635 10 319648 58 319848 2 319864 2 319891 9 319902 52 319955 6 320104 1 320147 8 320156 52 320403 59 320658 58 320903 5 320914 49 320967 3 321158 7 321170 49 321413 9 321425 49 321668 11 321681 49 321923 13 321937 49 322178 43 322226 16 322434 34 322470 6 322487 11 322689 31 322727 5 322745 9 322946 27 322983 3 323002 9 323203 24 323260 7 323460 21 323708 30 323963 31 324217 34 324472 35 324728 36 324985 35 325245 32 325504 27 325766 17 326030 6 346463 4 346719 10 346976 12 347232 13 347488 15 347545 1 347745 15 347800 4 348001 16 348055 7 348261 14 348310 8 348520 12 348565 9 348776 13 348820 11 349032 10 349075 12 349288 6 349330 14 349544 2 349585 15 349840 17 350097 16 350354 15 350611 7 350620 6 350868 6 350878 4 351125 5 351136 2 351382 4\",\n          \"307601 6 307850 18 308033 1 308057 2 308099 30 308287 3 308311 6 308348 42 308542 4 308564 8 308599 52 308796 7 308817 10 308853 58 309050 9 309071 11 309107 63 309305 10 309324 13 309361 68 309542 1 309559 12 309578 14 309615 73 309796 4 309814 14 309831 16 309869 79 310051 5 310068 16 310085 17 310123 84 310305 8 310323 34 310377 89 310560 9 310577 35 310631 94 310814 12 310831 36 310885 101 311069 14 311086 36 311139 103 311323 16 311340 37 311393 105 311578 55 311647 108 311832 61 311901 110 312087 65 312155 112 312341 181 312596 182 312852 181 313108 181 313364 180 313620 26 313648 102 313751 47 313876 21 313905 100 314009 43 314132 16 314161 99 314267 39 314389 12 314418 97 314525 4 314535 23 314645 11 314674 96 314799 9 314901 10 314929 96 315157 8 315183 97 315413 7 315438 97 315669 1 315671 4 315693 96 315928 2 315948 93 316203 90 316457 89 316712 86 316967 84 317221 84 317475 84 317728 85 317982 83 318236 62 318301 17 318492 62 318565 5 318748 62 319005 60 319261 57 319517 54 319774 51 320030 48 320287 44 320543 41 320801 36 321061 29 321321 22 321580 17 321848 2 340752 2 341008 4 341263 8 341519 10 341775 13 341838 1 342030 16 342093 3 342286 18 342349 3 342542 21 342604 5 342797 24 342859 7 343053 27 343114 8 343309 29 343370 9 343564 31 343625 11 343820 31 343880 12 344076 29 344135 14 344331 29 344391 15 344587 28 344646 16 344842 27 344901 18 345098 26 345156 20 345354 25 345412 20 345609 24 345667 22 345865 23 345924 19 346121 21 346181 16 346376 21 346438 13 346632 20 346695 10 346888 18 346952 7 347144 17 347209 4 347399 17 347466 1 347657 13 347915 10 348174 6 348432 2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-1c157ed5-e482-4b40-82fd-af54fdee4fa0\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>ClassId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002cc93b.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>29102 12 29346 24 29602 24 29858 24 30114 24 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0007a71bf.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>18661 28 18863 82 19091 110 19347 110 19603 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000a4bcdd.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>37607 3 37858 8 38108 14 38359 20 38610 25 388...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000f6bf48.jpg</td>\n",
       "      <td>4</td>\n",
       "      <td>131973 1 132228 4 132483 6 132738 8 132993 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0014fce06.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>229501 11 229741 33 229981 55 230221 77 230468...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c157ed5-e482-4b40-82fd-af54fdee4fa0')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-1c157ed5-e482-4b40-82fd-af54fdee4fa0 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-1c157ed5-e482-4b40-82fd-af54fdee4fa0');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-595931e2-4604-49fd-9e3d-4e3067d7cd0f\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-595931e2-4604-49fd-9e3d-4e3067d7cd0f')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-595931e2-4604-49fd-9e3d-4e3067d7cd0f button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "         ImageId  ClassId                                      EncodedPixels\n",
       "0  0002cc93b.jpg        1  29102 12 29346 24 29602 24 29858 24 30114 24 3...\n",
       "1  0007a71bf.jpg        3  18661 28 18863 82 19091 110 19347 110 19603 11...\n",
       "2  000a4bcdd.jpg        1  37607 3 37858 8 38108 14 38359 20 38610 25 388...\n",
       "3  000f6bf48.jpg        4  131973 1 132228 4 132483 6 132738 8 132993 11 ...\n",
       "4  0014fce06.jpg        3  229501 11 229741 33 229981 55 230221 77 230468..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbb192b7-a0ad-46c2-a28f-baa898a4f4aa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1758657947117,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "cbb192b7-a0ad-46c2-a28f-baa898a4f4aa",
    "include-cell-in-app": true,
    "outputId": "b7a1d500-4299-40b4-f6c2-c30aa3dce513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7095 entries, 0 to 7094\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   ImageId        7095 non-null   object\n",
      " 1   ClassId        7095 non-null   int64 \n",
      " 2   EncodedPixels  7095 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 166.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64a5a634-c7de-49b7-a62d-ac9c331e5ee3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1758657947152,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "64a5a634-c7de-49b7-a62d-ac9c331e5ee3",
    "include-cell-in-app": true,
    "outputId": "bc87a61b-2682-4ca3-cd64-8f1b78d949d4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ImageId</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ClassId</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EncodedPixels</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "ImageId          0\n",
       "ClassId          0\n",
       "EncodedPixels    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f25e41f-03f7-44eb-90e9-4da80bfbda7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1758668148316,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "0f25e41f-03f7-44eb-90e9-4da80bfbda7e",
    "include-cell-in-app": true,
    "outputId": "6b0c89a0-e5d3-4cd7-f4a4-69b1f5a8cb91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6666\n",
      "4\n",
      "ClassId\n",
      "2    0.034813\n",
      "4    0.112896\n",
      "1    0.126427\n",
      "3    0.725863\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check unique image numbers and classes\n",
    "image_ids = df['ImageId'].unique()\n",
    "print(len(image_ids))\n",
    "print(df['ClassId'].nunique())\n",
    "print(df['ClassId'].value_counts(normalize=True,ascending=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "Ji-1hVBiyl6h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1758668168082,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "Ji-1hVBiyl6h",
    "outputId": "8630b088-c030-4921-917e-d6b93744e02d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassId\n",
      "2     247\n",
      "4     801\n",
      "1     897\n",
      "3    5150\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['ClassId'].value_counts(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea278533-0dba-4bd3-be0f-775002c2c109",
   "metadata": {
    "id": "ea278533-0dba-4bd3-be0f-775002c2c109",
    "include-cell-in-app": true
   },
   "source": [
    "* There are a total of 6666 unique iamges in the training dataset\n",
    "* The defect classes are imbalanced, with class 2 comprising only 3.4% of the data, while class 3 accounts for 72.5%.\n",
    "* Augment the training data for the minority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ac8a56-b89a-4a44-bed8-e50a2853c536",
   "metadata": {
    "id": "12ac8a56-b89a-4a44-bed8-e50a2853c536",
    "include-cell-in-app": true
   },
   "source": [
    "## Prepare Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75f1260e-9d4a-4bc4-b2d8-7377ca6de590",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1758657947212,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "75f1260e-9d4a-4bc4-b2d8-7377ca6de590",
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "# RLE decoding function\n",
    "def rle_decode(mask_rle, shape=(256, 1600)):\n",
    "    if pd.isnull(mask_rle):\n",
    "        return np.zeros(shape, dtype=np.uint8)\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff47e719-c5de-42f2-af1d-210096fa3020",
   "metadata": {
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1758657947268,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "ff47e719-c5de-42f2-af1d-210096fa3020",
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "# Data augmentation function\n",
    "def augment_image_and_mask(img, mask):\n",
    "    # Random horizontal flip\n",
    "    if random.random() > 0.5:\n",
    "        img = np.fliplr(img)\n",
    "        mask = np.fliplr(mask)\n",
    "    # Random vertical flip\n",
    "    if random.random() > 0.5:\n",
    "        img = np.flipud(img)\n",
    "        mask = np.flipud(mask)\n",
    "    # Random brightness adjustment\n",
    "    if random.random() > 0.5:\n",
    "        factor = 0.7 + 0.6 * random.random()  # Range: 0.7 to 1.3\n",
    "        img = np.clip(img * factor, 0, 1)\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f13c3177-bf14-4eca-aeff-20682b7e15e4",
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1758657947308,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "f13c3177-bf14-4eca-aeff-20682b7e15e4",
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "# Custom Data Generator for efficient and safe multi-threaded data loading\n",
    "# Useful when working with large datasets that cannot fit entirely into memory.\n",
    "\n",
    "class SteelDataset(Sequence):\n",
    "    def __init__(self, df, img_dir, batch_size=16, img_shape=(128, 800), n_classes=4, shuffle=True, augmentation_factors=None,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.img_shape = img_shape\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.image_ids = self.df['ImageId'].unique()\n",
    "        self.augmentation_factors = augmentation_factors if augmentation_factors is not None else {1: 1, 2: 1, 3: 1, 4: 1}\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_ids) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_ids = self.image_ids[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "        X, y = [], []\n",
    "\n",
    "        for img_id in batch_ids:\n",
    "            img_path = os.path.join(self.img_dir, img_id)\n",
    "\n",
    "            # --- Debug: Check if file exists ---\n",
    "            if not os.path.exists(img_path):\n",
    "                raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            # --- Debug: Handle unreadable images ---\n",
    "            if img is None:\n",
    "                raise ValueError(f\"cv2.imread failed for {img_path}\")\n",
    "\n",
    "            img = cv2.resize(img, self.img_shape[::-1])\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "\n",
    "            mask = np.zeros((*self.img_shape, self.n_classes), dtype=np.uint8)\n",
    "\n",
    "            for c in range(1, self.n_classes+1):\n",
    "                rle = self.df[(self.df['ImageId'] == img_id) & (self.df['ClassId'] == c)]['EncodedPixels']\n",
    "                if len(rle) > 0 and isinstance(rle.values[0], str):\n",
    "                    mask[..., c-1] = rle_decode(rle.values[0], self.img_shape)\n",
    "\n",
    "            # ---augmentation ---\n",
    "            for c in [1, 2, 4]:\n",
    "                if mask[..., c-1].sum() > 0:\n",
    "                    for _ in range(self.augmentation_factors.get(c, 1) - 1):\n",
    "                        aug_img, aug_mask = augment_image_and_mask(img, mask)\n",
    "                        X.append(aug_img)\n",
    "                        y.append(aug_mask)\n",
    "\n",
    "            X.append(img)\n",
    "            y.append(mask)\n",
    "\n",
    "        return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.image_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zDbDOz9toJk5",
   "metadata": {
    "id": "zDbDOz9toJk5",
    "include-cell-in-app": true
   },
   "source": [
    "# Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Pk7ag7HMoXhH",
   "metadata": {
    "id": "Pk7ag7HMoXhH"
   },
   "source": [
    "## VGG-like Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "939de155-cebf-460c-a5c1-fdb525e9e2b4",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1758657947388,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "939de155-cebf-460c-a5c1-fdb525e9e2b4",
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "# VGG-like Model (Shallow)\n",
    "def build_vgg_like(input_shape=(128, 800, 3), n_classes=4):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2)(x)\n",
    "    x = layers.Conv2D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(256, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "    outputs = layers.Conv2D(n_classes, 1, activation='sigmoid', padding='same')(x)\n",
    "    model= models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NOHZjRctofAm",
   "metadata": {
    "id": "NOHZjRctofAm",
    "include-cell-in-app": true
   },
   "source": [
    "## Lightweight EfficientNet U-Net\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "-Lvaopd1iq7o",
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1758657947435,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "-Lvaopd1iq7o",
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "def build_lightweight_efficientnet_unet(input_shape=(128, 800, 3), n_classes=4):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Use EfficientNet as encoder\n",
    "    backbone = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_tensor=inputs\n",
    "    )\n",
    "\n",
    "    # Freeze backbone initially\n",
    "    backbone.trainable = False\n",
    "\n",
    "    # Get skip connections from EfficientNet\n",
    "    # These need to be carefully selected to match the decoder upsampling stages\n",
    "    skip1 = backbone.get_layer('block2a_expand_conv').output # Shape 64x400x96\n",
    "    skip2 = backbone.get_layer('block3a_expand_conv').output # Shape 32x200x144\n",
    "    skip3 = backbone.get_layer('block4a_expand_conv').output # Shape 16x100x240\n",
    "    skip4 = backbone.get_layer('block5a_expand_conv').output # Shape 8x50x672\n",
    "\n",
    "    # Start from backbone output (bottleneck)\n",
    "    x = backbone.output  #  4x25x1280\n",
    "\n",
    "    # Decoder with skip connections\n",
    "    x = layers.Conv2D(512, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.UpSampling2D(2)(x)  # 8x50x512\n",
    "    x = layers.concatenate([x, skip4]) # Concatenate with skip4 (8x50x672) -> 8x50x1184\n",
    "    x = layers.Conv2D(512, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.UpSampling2D(2)(x)  # 16x100x512\n",
    "    x = layers.concatenate([x, skip3]) # Concatenate with skip3 (16x100x240) -> 16x100x752\n",
    "    x = layers.Conv2D(256, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    x = layers.UpSampling2D(2)(x)  # 32x200x256\n",
    "    x = layers.concatenate([x, skip2]) # Concatenate with skip2 (32x200x144) -> 32x200x400\n",
    "    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "\n",
    "    x = layers.UpSampling2D(2)(x)  # 64x400x128\n",
    "    x = layers.concatenate([x, skip1]) # Concatenate with skip1 (64x400x96) -> 64x400x224\n",
    "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.UpSampling2D(2)(x)  # 128x800x64\n",
    "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Output\n",
    "    outputs = layers.Conv2D(n_classes, 1, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RQHMMx8EpD1K",
   "metadata": {
    "id": "RQHMMx8EpD1K",
    "include-cell-in-app": true
   },
   "source": [
    "## Metrics for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce71b9c0-f5fd-4a56-8bc2-3737ccdb5181",
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1758671152927,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "ce71b9c0-f5fd-4a56-8bc2-3737ccdb5181",
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "def iou_metric(y_true, y_pred, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true = tf.cast(y_true > 0.5, tf.float32)\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = tf.reduce_sum(y_true + y_pred, axis=[1,2,3]) - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return tf.reduce_mean(iou)\n",
    "\n",
    "def mean_dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"Calculate mean Dice coefficient across all classes\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "\n",
    "    dice_scores = []\n",
    "\n",
    "    # Calculate Dice for each class separately\n",
    "    for i in range(4):  # 4 classes\n",
    "        y_true_class = y_true[:, :, :, i]\n",
    "        y_pred_class = y_pred[:, :, :, i]\n",
    "\n",
    "        intersection = tf.reduce_sum(y_true_class * y_pred_class, axis=[1, 2])\n",
    "        union = tf.reduce_sum(y_true_class, axis=[1, 2]) + tf.reduce_sum(y_pred_class, axis=[1, 2])\n",
    "\n",
    "        dice = (2. * intersection + smooth) / (union + smooth)\n",
    "        dice_scores.append(dice)\n",
    "\n",
    "    # Stack and compute mean across classes, then mean across batch\n",
    "    dice_tensor = tf.stack(dice_scores, axis=1)  # Shape: (batch_size, 4)\n",
    "    mean_dice_per_sample = tf.reduce_mean(dice_tensor, axis=1)  # Shape: (batch_size,)\n",
    "\n",
    "    return tf.reduce_mean(mean_dice_per_sample)  # Scalar\n",
    "\n",
    "def dice_coefficient_per_class(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"Calculate Dice coefficient for each class individually (for debugging)\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "\n",
    "    # Calculate only for class 1 (can be modified for other classes)\n",
    "    y_true_class = y_true[:, :, :, 0]  # Class 1\n",
    "    y_pred_class = y_pred[:, :, :, 0]\n",
    "\n",
    "    intersection = tf.reduce_sum(y_true_class * y_pred_class, axis=[1, 2])\n",
    "    union = tf.reduce_sum(y_true_class, axis=[1, 2]) + tf.reduce_sum(y_pred_class, axis=[1, 2])\n",
    "\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "    return tf.reduce_mean(dice)\n",
    "\n",
    "def recall_metric(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"Calculate recall (sensitivity)\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "\n",
    "    true_positives = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    actual_positives = tf.reduce_sum(y_true, axis=[1,2,3])\n",
    "\n",
    "    recall = (true_positives + smooth) / (actual_positives + smooth)\n",
    "    return tf.reduce_mean(recall)\n",
    "\n",
    "def precision_metric(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"Calculate precision\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "\n",
    "    true_positives = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    predicted_positives = tf.reduce_sum(y_pred, axis=[1,2,3])\n",
    "\n",
    "    precision = (true_positives + smooth) / (predicted_positives + smooth)\n",
    "    return tf.reduce_mean(precision)\n",
    "\n",
    "def f1_score(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"Calculate F1 score (harmonic mean of precision and recall)\"\"\"\n",
    "    precision = precision_metric(y_true, y_pred, smooth)\n",
    "    recall = recall_metric(y_true, y_pred, smooth)\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + smooth)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def per_class_accuracy(y_true, y_pred):\n",
    "    \"\"\"Calculate accuracy per class, then average\"\"\"\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.float32)\n",
    "\n",
    "    class_accuracies = []\n",
    "    for i in range(4):\n",
    "        true_class = y_true[:, :, :, i]\n",
    "        pred_class = y_pred[:, :, :, i]\n",
    "        acc = tf.reduce_mean(tf.cast(tf.equal(true_class, pred_class), tf.float32))\n",
    "        class_accuracies.append(acc)\n",
    "\n",
    "    return tf.reduce_mean(class_accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af71c0cd-40fc-4265-9b92-6b2a82286a28",
   "metadata": {
    "include-cell-in-app": true
   },
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad51f31c-cc61-4355-8206-990703b3a433",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "# Custom loss\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1,2,3]) + tf.reduce_sum(y_pred, axis=[1,2,3])\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return 1 - tf.reduce_mean(dice)\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bXdAt9eHt8DV",
   "metadata": {
    "id": "bXdAt9eHt8DV",
    "include-cell-in-app": true
   },
   "source": [
    "# Model Training - VGG-like Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "523ef636-ae6f-497c-9865-c0e24fde7da9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 612828,
     "status": "ok",
     "timestamp": 1758671769099,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "523ef636-ae6f-497c-9865-c0e24fde7da9",
    "include-cell-in-app": true,
    "outputId": "6a022120-3f22-41d0-df97-7892c4690f40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 119ms/step - f1_score: 0.6820 - iou_metric: 0.5633 - loss: 1.0870 - mean_dice_coefficient: 0.8820 - per_class_accuracy: 0.9831 - precision_metric: 0.9860 - recall_metric: 0.5762 - val_f1_score: 0.7354 - val_iou_metric: 0.6163 - val_loss: 1.0352 - val_mean_dice_coefficient: 0.9022 - val_per_class_accuracy: 0.9887 - val_precision_metric: 1.0000 - val_recall_metric: 0.6163 - learning_rate: 5.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 87ms/step - f1_score: 0.6837 - iou_metric: 0.5623 - loss: 1.0533 - mean_dice_coefficient: 0.8865 - per_class_accuracy: 0.9860 - precision_metric: 1.0000 - recall_metric: 0.5623 - val_f1_score: 0.7354 - val_iou_metric: 0.6163 - val_loss: 1.0362 - val_mean_dice_coefficient: 0.9022 - val_per_class_accuracy: 0.9887 - val_precision_metric: 1.0000 - val_recall_metric: 0.6163 - learning_rate: 5.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 86ms/step - f1_score: 0.6956 - iou_metric: 0.5774 - loss: 1.0495 - mean_dice_coefficient: 0.8909 - per_class_accuracy: 0.9866 - precision_metric: 0.9998 - recall_metric: 0.5775 - val_f1_score: 0.7354 - val_iou_metric: 0.6163 - val_loss: 1.0343 - val_mean_dice_coefficient: 0.9022 - val_per_class_accuracy: 0.9887 - val_precision_metric: 1.0000 - val_recall_metric: 0.6163 - learning_rate: 5.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 110ms/step - f1_score: 0.6939 - iou_metric: 0.5762 - loss: 1.0493 - mean_dice_coefficient: 0.8910 - per_class_accuracy: 0.9859 - precision_metric: 1.0000 - recall_metric: 0.5762 - val_f1_score: 0.7354 - val_iou_metric: 0.6163 - val_loss: 1.0350 - val_mean_dice_coefficient: 0.9022 - val_per_class_accuracy: 0.9887 - val_precision_metric: 1.0000 - val_recall_metric: 0.6163 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "# Split train/val\n",
    "image_ids = df['ImageId'].unique()\n",
    "train_ids, val_ids = train_test_split(image_ids, test_size=0.1, random_state=42)\n",
    "train_df = df[df['ImageId'].isin(train_ids)]\n",
    "val_df = df[df['ImageId'].isin(val_ids)]\n",
    "# Get image IDs for each class for training data\n",
    "class_img_ids = {}\n",
    "for c in range(1, 5):\n",
    "    class_img_ids[c] = set(df[df['ClassId'] == c]['ImageId'].unique())\n",
    "augmentation_factors = {1: 2, 2: 6, 3: 1, 4: 2}\n",
    "# Generators\n",
    "train_gen = SteelDataset(train_df, '/content/drive/MyDrive/severstal-steel-defect-detection/train_images', batch_size=4, augmentation_factors=augmentation_factors)\n",
    "val_gen = SteelDataset(val_df, '/content/drive/MyDrive/severstal-steel-defect-detection/train_images', batch_size=4, shuffle=False)\n",
    "# Model\n",
    "model_vgg = build_vgg_like(input_shape=(128, 800, 3))\n",
    "# model_UNet = build_unet()\n",
    "model_vgg.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "                  loss=combined_loss,\n",
    "                  metrics=[iou_metric, mean_dice_coefficient, recall_metric, f1_score])\n",
    "# Add callbacks to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_iou_metric',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_iou_metric',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    min_lr=1e-6,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Train with callbacks\n",
    "history = model_vgg.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Save model\n",
    "# model_vgg.save('steel_defect_vgg.h5')\n",
    "# Save model to Drive\n",
    "# model_vgg.save('/content/drive/MyDrive/severstal-steel-defect-detection/steel_defect_vgg.h5')\n",
    "model_vgg.save('/content/drive/MyDrive/severstal-steel-defect-detection/steel_defect_vgg.keras')\n",
    "\n",
    "# Save history\n",
    "with open('/content/drive/MyDrive/severstal-steel-defect-detection/history_vgg.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2155bdb8-6130-41f3-a131-9a203ac0e841",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10353,
     "status": "ok",
     "timestamp": 1758672548301,
     "user": {
      "displayName": "Jing SHI",
      "userId": "08141155767914886912"
     },
     "user_tz": -60
    },
    "id": "2155bdb8-6130-41f3-a131-9a203ac0e841",
    "include-cell-in-app": true,
    "outputId": "b3723f36-bb21-432a-9359-b7b24fbed9f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m167/167\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 59ms/step - f1_score: 0.7296 - iou_metric: 0.6086 - loss: 1.0337 - mean_dice_coefficient: 0.9002 - per_class_accuracy: 0.9891 - precision_metric: 1.0000 - recall_metric: 0.6086\n",
      "Validation loss and metrics: [1.0351696014404297, 0.9886958599090576, 0.6162674427032471, 0.6162674427032471, 1.0, 0.7354426383972168, 0.9021956324577332]\n"
     ]
    }
   ],
   "source": [
    "# model = tf.keras.models.load_model('steel_defect_vgg.h5', custom_objects={'iou_metric': iou_metric})\n",
    "\n",
    "# Evaluate on the whole validation set\n",
    "results = model_vgg.evaluate(val_gen)\n",
    "print(\"Validation loss and metrics:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf55e02-6f9b-4b15-9b5e-83f176948dfe",
   "metadata": {
    "include-cell-in-app": true
   },
   "source": [
    "# Model Training - Lightweight EfficientNet U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27732ea-8369-4071-b0c0-76b99d7106c9",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "# Split train/val\n",
    "image_ids = df['ImageId'].unique()\n",
    "train_ids, val_ids = train_test_split(image_ids, test_size=0.1, random_state=42)\n",
    "train_df = df[df['ImageId'].isin(train_ids)]\n",
    "val_df = df[df['ImageId'].isin(val_ids)]\n",
    "# Get image IDs for each class for training data\n",
    "class_img_ids = {}\n",
    "for c in range(1, 5):\n",
    "    class_img_ids[c] = set(df[df['ClassId'] == c]['ImageId'].unique())\n",
    "augmentation_factors = {1: 2, 2: 6, 3: 1, 4: 2}\n",
    "# Generators\n",
    "train_gen = SteelDataset(train_df, '/content/drive/MyDrive/severstal-steel-defect-detection/train_images', batch_size=4, augmentation_factors=augmentation_factors)\n",
    "val_gen = SteelDataset(val_df, '/content/drive/MyDrive/severstal-steel-defect-detection/train_images', batch_size=4, shuffle=False)\n",
    "# Model\n",
    "model_unet = build_lightweight_efficientnet_unet()\n",
    "model_unet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                   loss=combined_loss,\n",
    "                   metrics= metrics=[iou_metric, mean_dice_coefficient, recall_metric, f1_score])\n",
    "\n",
    "# Add callbacks to prevent overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_iou_metric',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_iou_metric',\n",
    "    factor=0.5,\n",
    "    patience=2,\n",
    "    min_lr=1e-6,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Train with callbacks\n",
    "history = model_unet.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Save model\n",
    "# model_vgg.save('steel_defect_vgg.h5')\n",
    "# Save model to Drive\n",
    "# model_vgg.save('/content/drive/MyDrive/severstal-steel-defect-detection/steel_defect_vgg.h5')\n",
    "model_unet.save('/content/drive/MyDrive/severstal-steel-defect-detection/steel_defect_unet.keras')\n",
    "\n",
    "# Save history\n",
    "import pickle\n",
    "with open('/content/drive/MyDrive/severstal-steel-defect-detection/history_unet.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024d4a8f-9414-4373-b193-e32f5fee42e9",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "# After initial training, unfreeze backbone for fine-tuning\n",
    "print(\"Starting fine-tuning phase...\")\n",
    "model_unet.layers[1].trainable = True  # Unfreeze EfficientNet\n",
    "\n",
    "# Use much lower learning rate for fine-tuning\n",
    "model_unet.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),  # 10x lower LR\n",
    "    loss=combined_loss, \n",
    "    metrics=[iou_metric, mean_dice_coefficient, recall_metric, f1_score]\n",
    ")\n",
    "\n",
    "# Fine-tuning callbacks\n",
    "finetune_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_iou_metric',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_iou_metric',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        '/content/drive/MyDrive/severstal-steel-defect-detection/best_model_finetune.keras',\n",
    "        monitor='val_iou_metric',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Fine-tune the model\n",
    "history_finetune = model_unet.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=5,  # Fewer epochs for fine-tuning\n",
    "    verbose=1,\n",
    "    callbacks=finetune_callbacks\n",
    ")\n",
    "\n",
    "# Save final fine-tuned model\n",
    "model_unet.save('/content/drive/MyDrive/severstal-steel-defect-detection/steel_defect_unet_finetune.keras')\n",
    "\n",
    "# Save fine-tuning history\n",
    "with open('/content/drive/MyDrive/severstal-steel-defect-detection/history_unet_finetune.pkl', 'wb') as f:\n",
    "    pickle.dump(history_finetune.history, f)\n",
    "\n",
    "print(\"Fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d78b70-0bcb-430d-adb7-cb9ae56540bb",
   "metadata": {
    "include-cell-in-app": true
   },
   "source": [
    "# Models Comparison and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef6734e-0dd6-431e-a7d2-99c8c8119b5b",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "# Load histories from Google Drive\n",
    "with open('/content/drive/MyDrive/severstal-steel-defect-detection/history_vgg.pkl', 'rb') as f:\n",
    "    history_vgg = pickle.load(f)\n",
    "\n",
    "with open('/content/drive/MyDrive/severstal-steel-defect-detection/history_unet_finetune.pkl', 'rb') as f:\n",
    "    history_unet = pickle.load(f)\n",
    "\n",
    "# Plot training and validation loss side by side\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_vgg['loss'], label='Train Loss')\n",
    "plt.plot(history_vgg['val_loss'], label='Val Loss')\n",
    "plt.title('VGG-like Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_unet['loss'], label='Train Loss')\n",
    "plt.plot(history_unet['val_loss'], label='Val Loss')\n",
    "plt.title('EfficientNet U-Net Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c82865-2a33-4020-8264-8817a7f55e47",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "# Plot IoU\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_vgg['iou_metric'], label='Train IoU')\n",
    "plt.plot(history_vgg['val_iou_metric'], label='Val IoU')\n",
    "plt.title('VGG-like Model IoU')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_unet['iou_metric'], label='Train Loss')\n",
    "plt.plot(history_unet['val_iou_metric'], label='Val Loss')\n",
    "plt.title('EfficientNet U-Net IoU')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a4d32-5e59-4663-949f-d8b21e6b5797",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "# Plot Dice coefficient\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_vgg['mean_dice_coefficient'], label='Train Dice coefficient')\n",
    "plt.plot(history_vgg['val_mean_dice_coefficient'], label='Val Dice coefficient')\n",
    "plt.title('VGG-like Model Dice coefficient')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_unet['mean_dice_coefficient'], label='Train Dice coefficient')\n",
    "plt.plot(history_unet['val_mean_dice_coefficient'], label='Val Dice coefficient')\n",
    "plt.title('EfficientNet U-Net Dice coefficient')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8f1e6e-efe8-43ab-ae8f-69bebc5d5e25",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "vgg_model_path = 'steel_defect_vgg.keras'\n",
    "unet_model_path = 'steel_defect_unet_finetune.keras'\n",
    "\n",
    "# Custom objects for loading\n",
    "custom_objects = {\n",
    "    'iou_metric': iou_metric,\n",
    "    'mean_dice_coefficient': mean_dice_coefficient,\n",
    "    'recall_metric': recall_metric,\n",
    "    'dice_coefficient_per_class': dice_coefficient_per_class\n",
    "}\n",
    "\n",
    "# Load models\n",
    "vgg_model = tf.keras.models.load_model(vgg_model_path, custom_objects=custom_objects)\n",
    "unet_model = tf.keras.models.load_model(unet_model_path, custom_objects=custom_objects)\n",
    "\n",
    "# Evaluate models on val_gen\n",
    "vgg_results = vgg_model.evaluate(val_gen, verbose=1, return_dict=True)\n",
    "unet_results = unet_model.evaluate(val_gen, verbose=1, return_dict=True)\n",
    "\n",
    "# Extract metrics\n",
    "metrics = ['iou_metric','mean_dice_coefficient',  'recall_metric']\n",
    "vgg_metrics = [vgg_results[m] for m in metrics]\n",
    "unet_metrics = [unet_results[m] for m in metrics]\n",
    "\n",
    "# Plot comparison for each metric\n",
    "plt.figure(figsize=(10,4))\n",
    "bar_width = 0.35\n",
    "x = range(len(metrics))\n",
    "plt.bar([i - bar_width/2 for i in x], vgg_metrics, width=bar_width, label='VGG-like', color='skyblue')\n",
    "plt.bar([i + bar_width/2 for i in x], unet_metrics, width=bar_width, label='EfficientNet U-Net', color='salmon')\n",
    "plt.xticks(x, ['IoU', 'Mean Dice',  'Recall'])\n",
    "plt.ylabel('Score on Validation Set')\n",
    "plt.title('Model Comparison: IoU, Mean Dice, Recall')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584c11d6-829f-4412-b2e1-4635d604eb05",
   "metadata": {
    "include-cell-in-app": true
   },
   "outputs": [],
   "source": [
    "# evaluate the Dice coefficient for each class \n",
    "\n",
    "# get evaluation data\n",
    "X_val, y_val = [], []\n",
    "for i in range(len(val_gen)):\n",
    "    X_batch, y_batch = val_gen[i]\n",
    "    X_val.append(X_batch)\n",
    "    y_val.append(y_batch)\n",
    "X_val = np.concatenate(X_val, axis=0)\n",
    "y_val = np.concatenate(y_val, axis=0)\n",
    "\n",
    "# Compute Dice coefficient for each class for both models\n",
    "dice_vgg = []\n",
    "dice_unet = []\n",
    "for class_idx in range(4):\n",
    "    # VGG-like\n",
    "    y_pred_vgg = vgg_model.predict(X_val)\n",
    "    dice_vgg_class = dice_coefficient_per_class(\n",
    "        y_val[..., class_idx:class_idx+1], y_pred_vgg[..., class_idx:class_idx+1]\n",
    "    ).numpy()\n",
    "    dice_vgg.append(dice_vgg_class)\n",
    "    # EfficientNet U-Net\n",
    "    y_pred_unet = unet_model.predict(X_val)\n",
    "    dice_unet_class = dice_coefficient_per_class(\n",
    "        y_val[..., class_idx:class_idx+1], y_pred_unet[..., class_idx:class_idx+1]\n",
    "    ).numpy()\n",
    "    dice_unet.append(dice_unet_class)\n",
    "\n",
    "# Plot Dice coefficient per class\n",
    "classes = ['Class 1', 'Class 2', 'Class 3', 'Class 4']\n",
    "x = np.arange(len(classes))\n",
    "bar_width = 0.35\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(x - bar_width/2, dice_vgg, width=bar_width, label='VGG-like', color='skyblue')\n",
    "plt.bar(x + bar_width/2, dice_unet, width=bar_width, label='EfficientNet U-Net', color='salmon')\n",
    "plt.xticks(x, classes)\n",
    "plt.ylabel('Dice Coefficient')\n",
    "plt.title('Dice Coefficient per Class')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wd7OGFakmGoF",
   "metadata": {
    "id": "Wd7OGFakmGoF",
    "include-cell-in-app": true
   },
   "source": [
    "# Conclusions\n",
    "\n",
    "Two segmentation models—a VGG-like CNN and an EfficientNet U-Net—were trained on 6,666 steel plate images with four imbalanced defect classes. On-the-fly augmentation was applied to address imbalance. Both models achieved strong Dice scores (~0.90), but the VGG-like model delivered slightly higher IoU (0.62) and recall (0.73) while training faster.\n",
    "\n",
    "Despite EfficientNet U-Net’s pretrained encoder and skip connections, the simpler VGG-like model proved more effective. Likely causes include the domain gap between ImageNet pretraining and steel surface textures, and limited data leading to overfitting in the deeper model.\n",
    "\n",
    "Future work could explore deeper VGG variants with increased data augmentation to balance simplicity with higher representational capacity."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1t3kzwxOK78MeGgRg1nZFM7FQP7FHD5Fr",
     "timestamp": 1758579888524
    },
    {
     "file_id": "1AYu7g67uoGUEhRs9nJsaW08Bjs4VFE4d",
     "timestamp": 1758577833473
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
